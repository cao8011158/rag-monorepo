# Base reranker model (cross-encoder).
# Example (you choose one):
# - BAAI/bge-reranker-v2-m3
# - mixedbread-ai/mxbai-rerank-large-v1 (example name, use what you actually download)

model_name: "BAAI/bge-reranker-v2-m3"

stores:
  fs_local:
    kind: filesystem
    root: /content/drive/MyDrive/rag-kb-data

# ---------- DATA ----------
inputs:
  pairs:
    store: fs_local
    base: rq_out/pairs
    pairs: query_packs.jsonl

  chunks:
    store: fs_local
    base: ce_out/chunks
    chunks_file: chunks.jsonl

outputs:
  files:
    store: fs_local
    base: reranker_out
    train_path: "processed/train_query_pack.jsonl"
    valid_path: "processed/valid_query_pack.jsonl"
    train_pair_path: "processed/train_pair_epoch_{epoch}.jsonl"

# Tokenization
max_length: 512
# combine strategy:  "query_doc" means we encode as (query, doc) pair for cross-encoder.
pair_format: "query_doc"
data_split: 0.85

training:
  optimizer: AdamW
  output_dir: /content/drive/MyDrive/rag-kb-data/run1
  seed: 42
  num_epochs: 3
  # Pairwise sampling
  # Re-sample hard negatives each epoch:
  random_negative_per_positive: 1
  random_neg_ratio: 0.25

  lr: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.05

  per_device_train_batch_size: 8
  per_device_eval_batch_size: 20
  grad_accum_steps: 2

  log_every_steps: 20
  eval_every_steps: 200
  save_every_steps: 200

  max_steps: -1 # set integer to cap training

# Evaluation

eval:
  ndcg_k: 10
  mrr_k: 10
  infer_batch_size: 32
  max_negatives_per_query: 50

# QLoRA / LoRA
lora:
  enabled: true
  qlora_4bit: false
  r: 16
  alpha: 32
  dropout: 0.05
  # typical target modules for transformer blocks:
  target_modules: ["query", "key", "value", "dense"]

# Precision
bf16: true
fp16: false

# Misc
num_workers: 2
