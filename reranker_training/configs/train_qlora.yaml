# Base reranker model (cross-encoder).
# Example (you choose one):
# - BAAI/bge-reranker-v2-m3
# - mixedbread-ai/mxbai-rerank-large-v1 (example name, use what you actually download)
model_name: "BAAI/bge-reranker-v2-m3"

stores:
  fs_local:
    kind: filesystem
    root: /content/drive/MyDrive/rag-kb-data

# ---------- DATA ----------
inputs:
  pairs:
    store: fs_local
    base: rq_out/pairs
    pairs: query_pack.jsonl

  chunks:
    store: fs_local
    base: ce_out/chunks
    chunks_file: chunks.jsonl

outputs: 
  files:
    store: fs_local
    base: reranker_out
    train_path: "processed/train_query_pack.jsonl"
    valid_path: "processed/valid_query_pack.jsonl"
    train_pair_path: "processed/train_pair_epoch_n.jsonl"


# Tokenization
max_length: 512
# combine strategy:  "query_doc" means we encode as (query, doc) pair for cross-encoder.
pair_format: "query_doc"
data_split:0.85

Training:
  Optimizer: AdamW
  output_dir: "content/drive/MyDrive/rag-kb-data/run1"
  seed: 42
  num_epochs: 3
  # Pairwise sampling
  # Re-sample hard negatives each epoch:
  hard_negative_per_positive: 4
  random_negative_per_positive: 1
  random_neg_ratio: 0.25

  lr: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.05
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  grad_accum_steps: 2
  log_every_steps: 20
  eval_every_steps: 200
  save_every_steps: 200
  max_steps: null # set integer to cap training

# QLoRA / LoRA
lora:
  enabled: true
  qlora_4bit: true
  r: 16
  alpha: 32
  dropout: 0.05
  # typical target modules for transformer blocks:
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

# Precision
bf16: true
fp16: false

# Misc
num_workers: 2
